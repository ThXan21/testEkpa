{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAn example Intrusion Detection application using Dense, Conv1d and Lstm layers\nplease cite below works if you find it useful:\nAkgun, Devrim, Selman Hizal, and Unal Cavusoglu. \"A new DDoS attacks intrusion detection \nmodel based on deep learning for cybersecurity.\" Computers & Security 118 (2022): 102748.\n\nHizal, Selman, \u00dcnal \u00c7AVU\u015eO\u011eLU, and Devrim AKG\u00dcN. \"A New Deep Learning Based Intrusion \nDetection System for Cloud Security.\" 2021 3rd International Congress on Human-Computer \nInteraction, Optimization and Robotic Applications (HORA). IEEE, 2021.\n\"\"\"\n\n\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.utils import plot_model\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport os\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.utils import class_weight\nfrom models import models_ddos\n\n\nepochs = 100\nnclass = 12 \n\ndef loadDataset():\n    # Put dataset path here ! \n    filename='/mnt/data/pcap_data.csv'    \n\n    trainfile = pd.read_csv(filename)    \n    data = pd.DataFrame(trainfile).to_numpy()\n    data=data[data[:,67]!='DrDoS_LDAP']        \n    np.random.shuffle(data)\n    \n    label = data[:, 67].astype('str')\n    \n    label[label == 'WebDDoS']       = 0\n    label[label == 'BENIGN']        = 1\n    label[label == 'UDP-lag']       = 2\n    label[label == 'DrDoS_NTP']     = 3\n    label[label == 'Syn']           = 4\n    label[label == 'DrDoS_SSDP']    = 5\n    label[label == 'DrDoS_UDP']     = 6\n    label[label == 'DrDoS_NetBIOS'] = 7\n    label[label == 'DrDoS_MSSQL']   = 8\n    label[label == 'DrDoS_SNMP']    = 9\n    label[label == 'TFTP']          = 10\n    label[label == 'DrDoS_DNS']     = 11\n    #label[label == 'DrDoS_LDAP']     = 11\n   \n    # SELECT FEATURES ----------------------------------------------------\n    inx_sel=-1+np.array([38,47,37,48,11,9,7,52,10,36,1,34,4,17,19,57,21,\n                         18,22,24,32,50,23,55,51,5,3,39,40,43,58,12,25,\n                         20,2,35,67,33,6,53])\n    \n    # MIN-MAX normalization\n    data=data[:,inx_sel]\n    dmin = data.min(axis=0)\n    dmax = data.max(axis=0)\n    data=(data-dmin)/(dmax-dmin)\n    # data = np.log(data-dmin+1.0)    \n     \n\n    # Test data 20%\n    train_data, test_data, train_label, test_label = \\\n        train_test_split(data, label, test_size=0.20, stratify=label)\n        \n    # Train 70%, Validation%10\n    train_data, val_data, train_label, val_label = \\\n        train_test_split(train_data, train_label,test_size=0.125, stratify=train_label)\n\n\n    return train_data.astype('float32'), train_label.astype('int32'), \\\n        val_data.astype('float32'), val_label.astype('int32'), \\\n            test_data.astype('float32'), test_label.astype('int32')\n\n# -- LOAD DATA -----------------------------------------------------------------\ntrain_data, train_labelp, val_data, val_labelp, test_data, test_labelp = loadDataset()\n\n# to_categorical\ntrain_label = to_categorical(train_labelp, nclass)\nval_label   = to_categorical(val_labelp,   nclass)\ntest_label  = to_categorical(test_labelp,  nclass)\n\nprint('train_data.shape=', train_data.shape)\nprint('test_data.shape=',  test_data.shape)\nprint('test_data.shape=',  val_data.shape)\n\n#get the number of features\ninshape=train_data.shape[1]\n\n# Class balancing weights\nclass_weights = class_weight.compute_class_weight(class_weight='balanced',\n                                                  classes=np.unique(\n                                                      train_labelp),\n                                                  y=train_labelp)\n\n\nclass_weights = {i: class_weights[i] for i in range(len(class_weights))}\n\n\n# -- CALLBACKS -----------------------------------------------------------------\nearlyStopping = EarlyStopping(monitor='val_loss',\n                              patience=30,\n                              verbose=0,\n                              mode='min')\n\nmodelCheckPoint = ModelCheckpoint('./savemodels/model5class.weights.{epoch:03d}-{val_acc:.4f}.hdf5',\n                                  save_best_only=True,\n                                  monitor='val_acc',\n                                  mode='max')\n\n# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',\n#                                   factor=0.1,\n#                                   patience=7,\n#                                   verbose=1,\n#                                   epsilon=1e-4,\n#                                   mode='min')\n\n# -- Baseline models-----------------------------------------------------------\n\n# -- Conv1d\nmodel=models_ddos.model_conv1D(lr=1e-4,N=64,inshape=inshape)\n# -- Dense\n# model=models_ddos.model_dense(lr=1e-4,N=64,inshape=inshape)\n# -- LSTM\n# model=models_ddos.model_lstm(lr=1e-4,N=64,inshape=inshape)\n\nmodel.summary()\n# -----------------------------------------------------------------------------\n# print model to an image file\n# dot_img_file = 'model1.png'\n# plot_model(model, to_file=dot_img_file, show_shapes=True)\n\n# -- TRAIN MODEL --------------------------------------------------------------\nhistory = model.fit(train_data,\n                    train_label,\n                    shuffle=True,\n                    epochs=epochs,\n                    batch_size=256,  # 256,#128,#32, 64\n                    # validation_data=validation_generator,\n                    # validation_split=0.2,\n                    # validation_data=(val_data,val_label),\n                    validation_data=(val_data, val_label),\n                    callbacks=[modelCheckPoint],\n                    class_weight=class_weights,\n                    workers=3)\n\n# -- Load best model ----------------------------------------------------------\nstr_models = os.listdir('./savemodels')\nstr_models = np.sort(str_models)\nbest_model = str_models[str_models.size-1]\nprint('best_model=', best_model)\nmodel.load_weights('./savemodels/'+best_model)\n\n# --Confusion matrix ----------------------------------------------------------\nprint('TEST DATA-Confusion matrix:')  \npred = model.predict(test_data)\npred_y = pred.argmax(axis=-1)\n\ncm = confusion_matrix(test_labelp.astype('int32'), pred_y)\nprint(cm)\n\nprint('Accuracy ratios for each class')\nprint('WebDDoS      =', cm[0, 0]/np.sum(cm[0, :]))\nprint('BENIGN       =', cm[1, 1]/np.sum(cm[1, :]))\nprint('UDP-lag      =', cm[2, 2]/np.sum(cm[2, :]))\nprint('DrDoS_NTP    =', cm[3, 3]/np.sum(cm[3, :]))\nprint('Syn          =', cm[4, 4]/np.sum(cm[4, :]))\nprint('DrDoS_SSDP   =', cm[5, 5]/np.sum(cm[5, :]))\nprint('DrDoS_UDP    =', cm[6, 6]/np.sum(cm[6, :]))\nprint('DrDoS_NetBIOS=', cm[7, 7]/np.sum(cm[7, :]))\nprint('DrDoS_MSSQL  =', cm[8, 8]/np.sum(cm[8, :]))\nprint('DrDoS_SNMP   =', cm[9, 9]/np.sum(cm[9, :]))\nprint('TFTP         =', cm[10,10]/np.sum(cm[10, :]))\nprint('DrDoS_DNS    =', cm[11,11]/np.sum(cm[11, :]))\n\n\n# -- Confusion matrix plot\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nlabel=np.array([\"WebDDoS\",\"BENIGN\",\"UDP-lag\",\"DrDoS_NTP\",\"Syn \",\n                \"DrDoS_SSDP\",\"DrDoS_UDP\",\"DrDoS_NetBIOS\",\"DrDoS_MSSQL\",\n                \"DrDoS_SNMP\",\"TFTP\",\"DrDoS_DNS\"])\n\ncmo = ConfusionMatrixDisplay(cm,display_labels=label)\nfig, ax = plt.subplots(figsize=(12,12))\ncmo.plot(ax=ax, xticks_rotation=45)\n\n\n# Plot training and validation accurry and loss graphs\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nnp.save('historydata.npy',[acc,val_acc,loss,val_loss])\n[acc, val_acc, loss, val_loss] = np.load('historydata.npy')\n\nplt.figure()\nepochs = range(len(acc))\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r.', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r.', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 2}